{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertory = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ar41_for_ulb.csv'\n",
    "N = 2**13\n",
    "B = 2**6\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weather data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weather data from openweathermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# import json\n",
    "# import csv\n",
    "# import time\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # Timestamp 1 january 2023 UTC  is\t1672531201\n",
    "# # Each hour, + 3600\n",
    "# # Each day, + 86400\n",
    "# # End, 15 september, UTC is 1694818801\n",
    "# # Duration : 258 days or 22287600 seconds\n",
    "# # Careful, changement d'heure happening in Belgium but not for UTC\n",
    "\n",
    "# file = 'csvWeather.csv'\n",
    "# with open(repertory+file,'w',newline='') as weather :\n",
    "#     writer = csv.writer(weather)\n",
    "#     # Write the columns names on the csv\n",
    "#     fields = ['date','time','temp','wind','humidity']\n",
    "#     writer.writerow(fields)\n",
    "#     # For Bxl, Gand and Bastogne, write temperature, wind speed and humidity for each hour and each day between 01/01/23 and 15/09/23\n",
    "#     for day in range(258):\n",
    "#         for hour in range(24):\n",
    "#             urlbx = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.5&lon=4.20&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbx))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             dt = datetime.datetime.fromtimestamp(data['list'][0]['dt'])\n",
    "#             dt = str(dt).split()\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlgand = 'https://history.openweathermap.org/data/2.5/history/city?lat=51.03&lon=3.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlgand))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlbast = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.0&lon=5.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbast))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine our dataset with weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest point to define which weather report is the most relevant \n",
    "def closest(point, locs):\n",
    "    close = 9e20\n",
    "    for j in locs :\n",
    "        dist = 0\n",
    "        for i in range (2) :\n",
    "            dist += (point[i]-j[i])**2\n",
    "        if np.sqrt(dist) < close :\n",
    "            close = np.sqrt(dist)\n",
    "            best = j\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWeather = repertory+'csvWeather.csv'\n",
    "pathTrain = repertory+'ar41_for_ulb.csv'\n",
    "weather = pd.read_csv(pathWeather)\n",
    "train = pd.read_csv(pathTrain,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series for each dataframe that contains the closest point of the 3 studied locations\n",
    "# We chose 3 different towns in Belgium based on their average temperatures. \n",
    "# One in the south, rather cold, one in the center and one in the north, rather warm.\n",
    "locs = [(50.5, 4.2),(51.0,3.43),(50.0,5.43)]\n",
    "weather['closest'] = list(zip(weather['lat'], weather['lon']))\n",
    "train['coord'] = list(zip(train['lat'], train['lon']))\n",
    "train['closest'] = train.apply(lambda row: closest(row['coord'],locs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = train['timestamps_UTC'].str.split().str[0]\n",
    "train['time'] = train['timestamps_UTC'].str.split().str[1].str[:2]\n",
    "weather['time'] = pd.to_datetime(weather['time'], format='%H:%M:%S')\n",
    "weather['time'] = weather['time'].dt.hour.apply(lambda x: str(x).zfill(2))\n",
    "print(train.shape,weather.shape)\n",
    "final = pd.merge(train,weather,on=['time','date','closest'],how='inner')\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('final2.xlsx',engine='xlsxwriter') #TODO\n",
    "final2 = final[final['mapped_veh_id'] < 105]\n",
    "final2.to_excel(writer,sheet_name='sheet1')\n",
    "writer.close()\n",
    "final.to_csv(repertory+'trains_weather.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv with weather data\n",
    "file = 'trains_weather.csv'\n",
    "N = 17679273\n",
    "df = pd.read_csv(repertory+file, sep=';')\n",
    "df.info()\n",
    "df = df.iloc[:, :-2].drop(df.columns[16:18], axis=1) #remove some useless columns (closest,lat_y,lon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''use file to make fit & ruled_out through rule-based processing.\n",
    "fitting is not sorted by trains or by time'''\n",
    "\n",
    "def sep_temp_air(df) :\n",
    "    upper = 200\n",
    "    lower = -15 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = -5 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 200\n",
    "    lower = -100 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = -1\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 689\n",
    "    lower = 1 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "file = 'trains_weather.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+ file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "N = df.shape[0]\n",
    "df = df.dropna() #12000 NaN's in the PC2 columns\n",
    "print(\"Naaan : \", df.shape[0],N-df.shape[0])\n",
    "ruled_out = df.isna()\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.to_csv(repertory+'fittingtst.csv',sep=';',index=False)\n",
    "ruled_out.to_csv(repertory+'ruled_outtst.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fittingtst.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize time difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timezone\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "#remove nan\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "time_diffs = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "\n",
    "custom_bins = np.concatenate([np.arange(0, 130, 1)])\n",
    "\n",
    "# Plot a histogram of the time differences with custom bin edges\n",
    "plt.hist(time_diffs, bins=custom_bins, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Time Differences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time series by train and with regular interval of time between two consecutive timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "df.insert(3,'n_n1',n_n1)\n",
    "\n",
    "df2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\AppData\\Local\\Temp\\ipykernel_1164\\1622310166.py:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  now = pd.datetime.now(tz=pytz.UTC)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774710                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                 8  \n",
      "1      7  274.73  2.76        91  51.0  3.43                16  \n",
      "2      7  274.73  2.76        91  51.0  3.43                37  \n",
      "3      7  274.73  2.76        91  51.0  3.43                41  \n",
      "4      7  274.73  2.76        91  51.0  3.43                10  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                14  \n",
      "96    14  276.90  4.75        82  51.0  3.43                 8  \n",
      "97    14  276.90  4.75        82  51.0  3.43                14  \n",
      "98    14  276.90  4.75        82  51.0  3.43                 9  \n",
      "99    14  276.90  4.75        82  51.0  3.43                15  \n",
      "\n",
      "[100 rows x 27 columns]\n",
      "    traj            timestamps_UTC  n_n1\n",
      "0      0 2023-01-23 07:25:08+00:00   0.0\n",
      "1      0 2023-01-23 07:25:16+00:00   8.0\n",
      "2      0 2023-01-23 07:25:37+00:00  21.0\n",
      "3      0 2023-01-23 07:25:41+00:00   4.0\n",
      "4      0 2023-01-23 07:26:10+00:00  29.0\n",
      "..   ...                       ...   ...\n",
      "95     1 2023-01-23 14:52:14+00:00   6.0\n",
      "96     1 2023-01-23 14:53:08+00:00  54.0\n",
      "97     1 2023-01-23 14:53:14+00:00   6.0\n",
      "98     1 2023-01-23 14:54:09+00:00  55.0\n",
      "99     1 2023-01-23 14:54:15+00:00   6.0\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774710                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                 8  \n",
      "1      7  274.73  2.76        91  51.0  3.43                16  \n",
      "2      7  274.73  2.76        91  51.0  3.43                37  \n",
      "3      7  274.73  2.76        91  51.0  3.43                41  \n",
      "4      7  274.73  2.76        91  51.0  3.43                10  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                14  \n",
      "96    14  276.90  4.75        82  51.0  3.43                 8  \n",
      "97    14  276.90  4.75        82  51.0  3.43                14  \n",
      "98    14  276.90  4.75        82  51.0  3.43                 9  \n",
      "99    14  276.90  4.75        82  51.0  3.43                15  \n",
      "\n",
      "[100 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import timezone\n",
    "\n",
    "now = pd.datetime.now(tz=pytz.UTC)\n",
    "\n",
    "\n",
    "# Convert the timestamps_UTC column to datetime objects\n",
    "df2['timestamps_UTC'] = pd.to_datetime(df2['timestamps_UTC'], errors='coerce')\n",
    "\n",
    "# Create a new column 'bin_interval_time' containing the seconds part\n",
    "df2['bin_interval_time'] = df2['timestamps_UTC'].dt.second\n",
    "\n",
    "print(df2.head(100))\n",
    "\n",
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain2 = df2.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain2['sequence_start'] = sortedByTrain2['n_n1'] > 150 \n",
    "\n",
    "sortedByTrain2['traj'] = sortedByTrain2['sequence_start'].cumsum()\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1']].head(100))\n",
    "\n",
    "sortedByTrain2['most_common_bin'] = sortedByTrain2.groupby('traj')['bin_interval_time'].transform(lambda x: x.mode().iloc[0])\n",
    "\n",
    "print(df2.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.72 GiB for an array with shape (17, 13550319) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      6\u001b[0m sortedByTrain2 \u001b[38;5;241m=\u001b[39m sortedByTrain2[sortedByTrain2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiff_bin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tau]\n\u001b[1;32m----> 8\u001b[0m \u001b[43msortedByTrain2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_n1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\u001b[39;00m\n\u001b[0;32m     11\u001b[0m n_n1 \u001b[38;5;241m=\u001b[39m sortedByTrain2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamps_UTC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:4167\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4039\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4040\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4046\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4047\u001b[0m ):\n\u001b[0;32m   4048\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4049\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4050\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4165\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4169\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4173\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4174\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:3889\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   3888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3889\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:3924\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3923\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 3924\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3926\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   3927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3928\u001b[0m     labels \u001b[38;5;241m=\u001b[39m ensure_object(com\u001b[38;5;241m.\u001b[39mindex_labels_to_array(labels))\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\util\\_decorators.py:309\u001b[0m, in \u001b[0;36mrewrite_axis_style_signature.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any]:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:4036\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4034\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4035\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 4036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:4463\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   4462\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 4463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4464\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m   4465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3876\u001b[0m, in \u001b[0;36mDataFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   3874\u001b[0m columns \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3876\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\n\u001b[0;32m   3878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3880\u001b[0m index \u001b[38;5;241m=\u001b[39m axes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\frame.py:3921\u001b[0m, in \u001b[0;36mDataFrame._reindex_columns\u001b[1;34m(self, new_columns, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   3908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_reindex_columns\u001b[39m(\n\u001b[0;32m   3909\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3910\u001b[0m     new_columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3916\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3917\u001b[0m ):\n\u001b[0;32m   3918\u001b[0m     new_columns, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[0;32m   3919\u001b[0m         new_columns, method\u001b[38;5;241m=\u001b[39mmethod, level\u001b[38;5;241m=\u001b[39mlevel, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[0;32m   3920\u001b[0m     )\n\u001b[1;32m-> 3921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_with_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3922\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py:4526\u001b[0m, in \u001b[0;36mNDFrame._reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4523\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m ensure_int64(indexer)\n\u001b[0;32m   4525\u001b[0m \u001b[38;5;66;03m# TODO: speed up on homogeneous DataFrame objects\u001b[39;00m\n\u001b[1;32m-> 4526\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4529\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_dups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4533\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4534\u001b[0m \u001b[38;5;66;03m# If we've made a copy once, no need to make another one\u001b[39;00m\n\u001b[0;32m   4535\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1291\u001b[0m, in \u001b[0;36mBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate)\u001b[0m\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1291\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1293\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1294\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1295\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1302\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1419\u001b[0m, in \u001b[0;36mBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice)\u001b[0m\n\u001b[0;32m   1417\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m   1418\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1419\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1420\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1255\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1255\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\algorithms.py:1732\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1730\u001b[0m         out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1732\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1734\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m   1735\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m   1736\u001b[0m )\n\u001b[0;32m   1737\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.72 GiB for an array with shape (17, 13550319) and data type float64"
     ]
    }
   ],
   "source": [
    "sortedByTrain = sortedByTrain2\n",
    "sortedByTrain2[\"diff_bin\"] = abs(sortedByTrain2[\"bin_interval_time\"] - sortedByTrain2[\"most_common_bin\"])\n",
    "\n",
    "tau = 10\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2[sortedByTrain2[\"diff_bin\"] <= tau]\n",
    "\n",
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2[sortedByTrain2[\"n_n1\"] <= 2 * tau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['level_0', 'index', 'Unnamed: 0.1', 'mapped_veh_id', 'timestamps_UTC',\n",
      "       'lat_x', 'lon_x', 'RS_E_InAirTemp_PC1', 'RS_E_InAirTemp_PC2',\n",
      "       'RS_E_OilPress_PC1', 'RS_E_OilPress_PC2', 'RS_E_RPM_PC1',\n",
      "       'RS_E_RPM_PC2', 'RS_E_WatTemp_PC1', 'RS_E_WatTemp_PC2',\n",
      "       'RS_T_OilTemp_PC1', 'RS_T_OilTemp_PC2', 'coord', 'closest', 'temp',\n",
      "       'wind', 'humidity', 'lat_y', 'lon_y', 'bin_interval_time', 'traj',\n",
      "       'most_common_bin'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tau\n",
    "tau = 10\n",
    "\n",
    "# Create a new dataframe for the regular time intervals\n",
    "df_regular_intervals = pd.DataFrame(columns=sortedByTrain2.columns)\n",
    "\n",
    "# Iterate through each row in the original dataframe\n",
    "for i in range(len(sortedByTrain2)):\n",
    "    current_row = sortedByTrain2.iloc[i]\n",
    "\n",
    "    # Check the condition for the timestamps_UTC difference\n",
    "    if i == 0 or (current_row['timestamps_UTC'] - prev_row['timestamps_UTC']).seconds in range(60 - tau, 60 + tau + 1):\n",
    "        \n",
    "        # Check if the timestamps_UTC is close to most_common_bin\n",
    "        bin_diff = abs((current_row['timestamps_UTC'] - pd.Timestamp(current_row['most_common_bin'])).seconds)\n",
    "        if bin_diff <= tau:\n",
    "            df_regular_intervals = df_regular_intervals.append(current_row, ignore_index=True)\n",
    "\n",
    "    # Update the previous row\n",
    "    prev_row = current_row\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df_regular_intervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = sortedByTrain2.copy()\n",
    "tau = 10\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['bin_interval_time'] >= sortedByTrain2['most_common_bin']-tau) & (sortedByTrain2['bin_interval_time'] <= sortedByTrain2['most_common_bin']+tau))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.to_csv('./result/clean1.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'clean1.csv'\n",
    "\n",
    "sortedByTrain2 = pd.read_csv(repertory+file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamps_UTC column to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], errors='coerce')\n",
    "\n",
    "# Create a new column 'seconds' containing the seconds part\n",
    "sortedByTrain2['seconds'] = sortedByTrain2['timestamps_UTC'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\",\"seconds\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop time intervals \n",
    "tau = 10\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns=sortedByTrain2.columns)\n",
    "print(df_final)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df_final),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "    \n",
    "def addrow(row,df_final) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df_final.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df_final.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df_final.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain2.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[sortedByTrain2[\"traj\"]==20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2 = df3\n",
    "tau = 12\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['n_n1'] >= 60-tau) & (sortedByTrain2['n_n1'] <= 60+tau)) | (sortedByTrain2['n_n1'] >= 120-tau) & (sortedByTrain2['n_n1'] <= 120+tau)]\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop time intervals \n",
    "tau = 12\n",
    "print(df.head(100))\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.head(100))\n",
    "df.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain['timestamps_UTC'] = pd.to_datetime(sortedByTrain['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain['sequence_start'] = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain['traj'] = sortedByTrain['sequence_start'].cumsum()\n",
    "print(sortedByTrain['traj'].drop_duplicates())\n",
    "sortedByTrain = sortedByTrain.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(502))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=sortedByTrain.columns)\n",
    "print(df3)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df3),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "def addrow(row,df3) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df3.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df3.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df3.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.shape)\n",
    "df = pd.concat([sortedByTrain,df3])\n",
    "print(sortedByTrain.shape)\n",
    "print(df.shape)\n",
    "df=df.interpolate()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain.to_csv(dirpath/'trajets_train.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain = sortedByTrain[['mapped_veh_id','timestamps_UTC','traj']]\n",
    "meantrajlength = sortedByTrain.groupby('traj').count().agg('mean')\n",
    "print(meantrajlength)\n",
    "print(sortedByTrain[sortedByTrain['traj'] == 183182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of time series (we consider a new time series when, between two timestamps, the train change or the difference of time is greater than 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Use tqdm to add a progress bar\n",
    "for i in tqdm(range(1, len(df)), desc=\"Processing Rows\", unit=\" row\"):\n",
    "    # Check if it's the same train\n",
    "    if df['mapped_veh_id'].iloc[i] == df['mapped_veh_id'].iloc[i - 1]:\n",
    "        # Calculate time difference in seconds\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds()\n",
    "\n",
    "        # Check for gaps greater than 5 minutes\n",
    "        if time_diff > 300:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc(i))\n",
    "    # Create new time series if it is a new train\n",
    "    else : \n",
    "        if current_time_series:\n",
    "            time_series_list.append(current_time_series)\n",
    "        current_time_series = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Iterate over possible starting points (offsets)\n",
    "max_timestamps = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in tqdm(range(60), desc=\"Searching for Best Offset\", unit=\"second\"):\n",
    "    current_time_series = []\n",
    "    current_time_series.append(df.head(1))\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for i in range(1, len(df)):\n",
    "        # Calculate time difference in seconds with the offset\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds() + offset\n",
    "\n",
    "        # Check for gaps greater than 60 seconds\n",
    "        if time_diff > 60:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc[i])\n",
    "\n",
    "    # Create new time series if it is a new train\n",
    "    if current_time_series:\n",
    "        time_series_list.append(current_time_series)\n",
    "\n",
    "    # Update best offset if the current offset gives more timestamps\n",
    "    if len(current_time_series) > max_timestamps:\n",
    "        max_timestamps = len(current_time_series)\n",
    "        best_offset = offset\n",
    "\n",
    "print(f\"Best Offset: {best_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in time_series_list:\n",
    "    for i in range(0, 59):\n",
    "        \n",
    "        for df_chunk in time_series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and sort dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_temp_air(df) :\n",
    "    upper = 80\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = 0\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 600\n",
    "    lower = 100 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "\n",
    "def sep_cap_diff(df) :\n",
    "    column_name=df.columns[4:]\n",
    "    sensor_columns = column_name\n",
    "    sensor_columnsPC1 = sensor_columns[::2]\n",
    "    sensor_columnsPC2 = sensor_columns[1::2]\n",
    "    reldiff = 0.5\n",
    "    diff = pd.DataFrame()\n",
    "    allzero = pd.DataFrame()\n",
    "    fitting = df.copy()\n",
    "    for i in range(len(sensor_columnsPC1)):    \n",
    "        close = (np.isclose(fitting[sensor_columnsPC1[i]], \\\n",
    "                           fitting[sensor_columnsPC2[i]], rtol=reldiff) & (fitting[sensor_columnsPC1[i]] != 0) & (fitting[sensor_columnsPC2[i]] != 0))\n",
    "        invalid = ((fitting[sensor_columnsPC1[i]] == 0) & (fitting[sensor_columnsPC2[i]] == 0))\n",
    "        diff = diff._append(fitting[~close])\n",
    "        allzero = allzero._append(fitting[invalid])\n",
    "        fitting = fitting[close & ~invalid]\n",
    "        print(\"dshape : \",  diff.shape)\n",
    "    diff['different'] = True\n",
    "    allzero['allzero'] = True\n",
    "    return fitting, diff, allzero\n",
    "    \n",
    "df = pd.read_csv(file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = h\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "\n",
    "fit, diff, az = sep_cap_diff(fit)\n",
    "print(\"Sensor diff : \",fit.shape, diff.shape, az.shape)\n",
    "ruled_out= ruled_out._append(diff)\n",
    "ruled_out= ruled_out._append(az)\n",
    "ruled_out = ruled_out[~ruled_out.index.duplicated(keep='first')]\n",
    "print(\"ruled out df : \",ruled_out.shape)\n",
    "\n",
    "fit.to_csv(dirpath/'fitting.csv',sep=';')\n",
    "ruled_out.to_csv(dirpath/'ruled_out.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamps_UTC' column to datetime type\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# Sort the DataFrame by the 'timestamps_UTC' column\n",
    "df_sorted = df.sort_values(by='timestamps_UTC')\n",
    "\n",
    "# new_file = 'sorted_ar41_for_ulb.csv'\n",
    "\n",
    "# # Save the sorted DataFrame to a new CSV file\n",
    "# df_sorted.to_csv(repertory+new_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sorted = df\n",
    "# Select the first 100 rows\n",
    "df_subset = df_sorted.head(N)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_column = df_subset.isnull().sum()\n",
    "\n",
    "# Check if there are any missing values in the entire DataFrame\n",
    "any_missing_values = df_subset.isnull().values.any()\n",
    "\n",
    "total_missing_values = missing_values_column.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing values in each column:\\n\", missing_values_column)\n",
    "print(\"\\nAre there any missing values in the DataFrame?\", any_missing_values)\n",
    "print(\"Nombre total de valeurs manquantes dans le DataFrame:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_subset = df_subset.iloc[:1000]\n",
    "\n",
    "current_set = df_sub_subset\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = current_set.interpolate()\n",
    "\n",
    "# Check if there are any missing values after interpolation\n",
    "any_missing_values_after_interpolation = df_interpolated.isnull().values.any()\n",
    "\n",
    "# Display the results\n",
    "print(\"Are there any missing values in the DataFrame after interpolation?\", any_missing_values_after_interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values using polynomial interpolation of order 2\n",
    "df_interpolated_polynomial = current_set.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Original Data\n",
    "plt.scatter(range(len(current_set)), current_set['RS_E_InAirTemp_PC2'], label='Original', marker='o', color='blue', alpha=0.5)\n",
    "\n",
    "# Linear Interpolation\n",
    "# plt.plot(range(len(df_interpolated)), df_interpolated['RS_E_InAirTemp_PC2'], label='Linear Interpolation', color='green')\n",
    "\n",
    "# Polynomial Interpolation\n",
    "plt.plot(range(len(df_interpolated_polynomial)), df_interpolated_polynomial['RS_E_InAirTemp_PC2'], label='Polynomial Interpolation', color='orange')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('RS_E_InAirTemp_PC2')\n",
    "plt.title('Comparison of Interpolations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_linear = current_set['RS_E_InAirTemp_PC2'] - df_interpolated['RS_E_InAirTemp_PC2']\n",
    "residuals_polynomial = current_set['RS_E_InAirTemp_PC2'] - df_interpolated_polynomial['RS_E_InAirTemp_PC2']\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(len(current_set)), residuals_linear, label='Linear Residuals', color='green')\n",
    "plt.plot(range(len(current_set)), residuals_polynomial, label='Polynomial Residuals', color='orange')\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1, label='Zero Residual')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['mapped_veh_id'].nunique()\n",
    "print(f'Total number of unique values: {unique_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "def plot_autocorrelation(df, column_name, lags=40):\n",
    "    \"\"\"\n",
    "    Plot autocorrelation for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - column_name (str): The name of the column for which to plot the autocorrelation.\n",
    "    - lags (int): Number of lags to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(df[column_name], lags=lags)\n",
    "    plt.title(f'Autocorrelation Plot for {column_name}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "selected_columns = current_set.columns[5:]\n",
    "print(selected_columns)\n",
    "\n",
    "# selected_columns = current_set.columns[6]\n",
    "for column_name in selected_columns:\n",
    "    plot_autocorrelation(current_set, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stationarity or non stationarity of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from 5th to 8th (index 4 to 7)\n",
    "selected_columns = current_set.columns[5:]\n",
    "\n",
    "for column_name in selected_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(current_set.index, current_set[column_name], label=column_name)\n",
    "    plt.title(f'Data Plot for {column_name}')\n",
    "    plt.xlabel('Index')  # Use 'Index' instead of 'Timestamp'\n",
    "    plt.ylabel(column_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
