{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertory = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ar41_for_ulb.csv'\n",
    "N = 2**13\n",
    "B = 2**6\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weather data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weather data from openweathermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# import json\n",
    "# import csv\n",
    "# import time\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # Timestamp 1 january 2023 UTC  is\t1672531201\n",
    "# # Each hour, + 3600\n",
    "# # Each day, + 86400\n",
    "# # End, 15 september, UTC is 1694818801\n",
    "# # Duration : 258 days or 22287600 seconds\n",
    "# # Careful, changement d'heure happening in Belgium but not for UTC\n",
    "\n",
    "# file = 'csvWeather.csv'\n",
    "# with open(repertory+file,'w',newline='') as weather :\n",
    "#     writer = csv.writer(weather)\n",
    "#     # Write the columns names on the csv\n",
    "#     fields = ['date','time','temp','wind','humidity']\n",
    "#     writer.writerow(fields)\n",
    "#     # For Bxl, Gand and Bastogne, write temperature, wind speed and humidity for each hour and each day between 01/01/23 and 15/09/23\n",
    "#     for day in range(258):\n",
    "#         for hour in range(24):\n",
    "#             urlbx = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.5&lon=4.20&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbx))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             dt = datetime.datetime.fromtimestamp(data['list'][0]['dt'])\n",
    "#             dt = str(dt).split()\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlgand = 'https://history.openweathermap.org/data/2.5/history/city?lat=51.03&lon=3.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlgand))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlbast = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.0&lon=5.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbast))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine our dataset with weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest point to define which weather report is the most relevant \n",
    "def closest(point, locs):\n",
    "    close = 9e20\n",
    "    for j in locs :\n",
    "        dist = 0\n",
    "        for i in range (2) :\n",
    "            dist += (point[i]-j[i])**2\n",
    "        if np.sqrt(dist) < close :\n",
    "            close = np.sqrt(dist)\n",
    "            best = j\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWeather = repertory+'csvWeather.csv'\n",
    "pathTrain = repertory+'ar41_for_ulb.csv'\n",
    "weather = pd.read_csv(pathWeather)\n",
    "train = pd.read_csv(pathTrain,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series for each dataframe that contains the closest point of the 3 studied locations\n",
    "# We chose 3 different towns in Belgium based on their average temperatures. \n",
    "# One in the south, rather cold, one in the center and one in the north, rather warm.\n",
    "locs = [(50.5, 4.2),(51.0,3.43),(50.0,5.43)]\n",
    "weather['closest'] = list(zip(weather['lat'], weather['lon']))\n",
    "train['coord'] = list(zip(train['lat'], train['lon']))\n",
    "train['closest'] = train.apply(lambda row: closest(row['coord'],locs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = train['timestamps_UTC'].str.split().str[0]\n",
    "train['time'] = train['timestamps_UTC'].str.split().str[1].str[:2]\n",
    "weather['time'] = pd.to_datetime(weather['time'], format='%H:%M:%S')\n",
    "weather['time'] = weather['time'].dt.hour.apply(lambda x: str(x).zfill(2))\n",
    "print(train.shape,weather.shape)\n",
    "final = pd.merge(train,weather,on=['time','date','closest'],how='inner')\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('final2.xlsx',engine='xlsxwriter') #TODO\n",
    "final2 = final[final['mapped_veh_id'] < 105]\n",
    "final2.to_excel(writer,sheet_name='sheet1')\n",
    "writer.close()\n",
    "final.to_csv(repertory+'trains_weather.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv with weather data\n",
    "file = 'trains_weather.csv'\n",
    "N = 17679273\n",
    "df = pd.read_csv(repertory+file, sep=';')\n",
    "df.info()\n",
    "df = df.iloc[:, :-2].drop(df.columns[16:18], axis=1) #remove some useless columns (closest,lat_y,lon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''use file to make fit & ruled_out through rule-based processing.\n",
    "fitting is not sorted by trains or by time'''\n",
    "\n",
    "def sep_temp_air(df) :\n",
    "    upper = 200\n",
    "    lower = -15 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = -5 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 200\n",
    "    lower = -100 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = -1\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 689\n",
    "    lower = 1 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "file = 'trains_weather.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+ file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "N = df.shape[0]\n",
    "df = df.dropna() #12000 NaN's in the PC2 columns\n",
    "print(\"Naaan : \", df.shape[0],N-df.shape[0])\n",
    "ruled_out = df.isna()\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.to_csv(repertory+'fittingtst.csv',sep=';',index=False)\n",
    "ruled_out.to_csv(repertory+'ruled_outtst.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fittingtst.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize time difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timezone\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "#remove nan\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "time_diffs = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "\n",
    "custom_bins = np.concatenate([np.arange(0, 130, 1)])\n",
    "\n",
    "# Plot a histogram of the time differences with custom bin edges\n",
    "plt.hist(time_diffs, bins=custom_bins, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Time Differences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time series by train and with regular interval of time between two consecutive timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "df.insert(3,'n_n1',n_n1)\n",
    "\n",
    "df2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from datetime import timezone\n",
    "\n",
    "now = pd.datetime.now(tz=pytz.UTC)\n",
    "\n",
    "\n",
    "df2['bin_interval_time'] = ((df2['timestamps_UTC'] - now).dt.total_seconds() % 60).astype(int) #TODO check\n",
    "print(df2.head(100))\n",
    "\n",
    "\n",
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain2 = df2.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain2['sequence_start'] = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain2['traj'] = sortedByTrain2['sequence_start'].cumsum()\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1']].head(100))\n",
    "\n",
    "sortedByTrain2['most_common_bin'] = sortedByTrain2.groupby('traj')['bin_interval_time'].transform(lambda x: x.mode().iloc[0])\n",
    "\n",
    "print(df2.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = sortedByTrain2.copy()\n",
    "tau = 10\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['bin_interval_time'] >= sortedByTrain2['most_common_bin']-tau) & (sortedByTrain2['bin_interval_time'] <= sortedByTrain2['most_common_bin']+tau))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.to_csv('./result/clean1.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'clean1.csv'\n",
    "\n",
    "sortedByTrain2 = pd.read_csv(repertory+file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamps_UTC column to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], errors='coerce')\n",
    "\n",
    "# Create a new column 'seconds' containing the seconds part\n",
    "sortedByTrain2['seconds'] = sortedByTrain2['timestamps_UTC'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timestamps_UTC    n_n1  bin_interval_time  most_common_bin  \\\n",
      "5945303 2023-06-02 14:03:30+00:00  2109.0                 45               45   \n",
      "5945304 2023-06-02 14:04:30+00:00    60.0                 45               45   \n",
      "5945305 2023-06-02 14:05:30+00:00    60.0                 45               45   \n",
      "5945306 2023-06-02 14:06:30+00:00    60.0                 45               45   \n",
      "5945307 2023-06-02 14:06:40+00:00    10.0                 55               45   \n",
      "5945308 2023-06-02 14:07:30+00:00    50.0                 45               45   \n",
      "5945309 2023-06-02 14:07:31+00:00     1.0                 46               45   \n",
      "5945310 2023-06-02 14:08:31+00:00    60.0                 46               45   \n",
      "5945311 2023-06-02 14:08:34+00:00     3.0                 49               45   \n",
      "5945312 2023-06-02 14:09:21+00:00    47.0                 36               45   \n",
      "5945313 2023-06-02 14:09:34+00:00    13.0                 49               45   \n",
      "5945314 2023-06-02 14:10:24+00:00    50.0                 39               45   \n",
      "5945315 2023-06-02 14:10:34+00:00    10.0                 49               45   \n",
      "5945316 2023-06-02 14:11:25+00:00    51.0                 40               45   \n",
      "5945317 2023-06-02 14:11:34+00:00     9.0                 49               45   \n",
      "5945318 2023-06-02 14:12:25+00:00    51.0                 40               45   \n",
      "5945319 2023-06-02 14:12:35+00:00    10.0                 50               45   \n",
      "5945320 2023-06-02 14:13:25+00:00    50.0                 40               45   \n",
      "5945321 2023-06-02 14:13:27+00:00     2.0                 42               45   \n",
      "5945322 2023-06-02 14:14:25+00:00    58.0                 40               45   \n",
      "5945323 2023-06-02 14:14:27+00:00     2.0                 42               45   \n",
      "5945324 2023-06-02 14:15:27+00:00    60.0                 42               45   \n",
      "5945325 2023-06-02 14:15:28+00:00     1.0                 43               45   \n",
      "5945326 2023-06-02 14:17:28+00:00   120.0                 43               45   \n",
      "5945327 2023-06-02 14:18:29+00:00    61.0                 44               45   \n",
      "\n",
      "         seconds  \n",
      "5945303       30  \n",
      "5945304       30  \n",
      "5945305       30  \n",
      "5945306       30  \n",
      "5945307       40  \n",
      "5945308       30  \n",
      "5945309       31  \n",
      "5945310       31  \n",
      "5945311       34  \n",
      "5945312       21  \n",
      "5945313       34  \n",
      "5945314       24  \n",
      "5945315       34  \n",
      "5945316       25  \n",
      "5945317       34  \n",
      "5945318       25  \n",
      "5945319       35  \n",
      "5945320       25  \n",
      "5945321       27  \n",
      "5945322       25  \n",
      "5945323       27  \n",
      "5945324       27  \n",
      "5945325       28  \n",
      "5945326       28  \n",
      "5945327       29  \n"
     ]
    }
   ],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\",\"seconds\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop time intervals \n",
    "tau = 10\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns=sortedByTrain2.columns)\n",
    "print(df_final)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df_final),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "    \n",
    "def addrow(row,df_final) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df_final.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df_final.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df_final.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain2.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[sortedByTrain2[\"traj\"]==20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2 = df3\n",
    "tau = 12\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['n_n1'] >= 60-tau) & (sortedByTrain2['n_n1'] <= 60+tau)) | (sortedByTrain2['n_n1'] >= 120-tau) & (sortedByTrain2['n_n1'] <= 120+tau)]\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop time intervals \n",
    "tau = 12\n",
    "print(df.head(100))\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.head(100))\n",
    "df.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain['timestamps_UTC'] = pd.to_datetime(sortedByTrain['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain['sequence_start'] = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain['traj'] = sortedByTrain['sequence_start'].cumsum()\n",
    "print(sortedByTrain['traj'].drop_duplicates())\n",
    "sortedByTrain = sortedByTrain.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(502))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=sortedByTrain.columns)\n",
    "print(df3)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df3),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "def addrow(row,df3) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df3.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df3.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df3.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.shape)\n",
    "df = pd.concat([sortedByTrain,df3])\n",
    "print(sortedByTrain.shape)\n",
    "print(df.shape)\n",
    "df=df.interpolate()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain.to_csv(dirpath/'trajets_train.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain = sortedByTrain[['mapped_veh_id','timestamps_UTC','traj']]\n",
    "meantrajlength = sortedByTrain.groupby('traj').count().agg('mean')\n",
    "print(meantrajlength)\n",
    "print(sortedByTrain[sortedByTrain['traj'] == 183182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of time series (we consider a new time series when, between two timestamps, the train change or the difference of time is greater than 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Use tqdm to add a progress bar\n",
    "for i in tqdm(range(1, len(df)), desc=\"Processing Rows\", unit=\" row\"):\n",
    "    # Check if it's the same train\n",
    "    if df['mapped_veh_id'].iloc[i] == df['mapped_veh_id'].iloc[i - 1]:\n",
    "        # Calculate time difference in seconds\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds()\n",
    "\n",
    "        # Check for gaps greater than 5 minutes\n",
    "        if time_diff > 300:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc(i))\n",
    "    # Create new time series if it is a new train\n",
    "    else : \n",
    "        if current_time_series:\n",
    "            time_series_list.append(current_time_series)\n",
    "        current_time_series = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Iterate over possible starting points (offsets)\n",
    "max_timestamps = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in tqdm(range(60), desc=\"Searching for Best Offset\", unit=\"second\"):\n",
    "    current_time_series = []\n",
    "    current_time_series.append(df.head(1))\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for i in range(1, len(df)):\n",
    "        # Calculate time difference in seconds with the offset\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds() + offset\n",
    "\n",
    "        # Check for gaps greater than 60 seconds\n",
    "        if time_diff > 60:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc[i])\n",
    "\n",
    "    # Create new time series if it is a new train\n",
    "    if current_time_series:\n",
    "        time_series_list.append(current_time_series)\n",
    "\n",
    "    # Update best offset if the current offset gives more timestamps\n",
    "    if len(current_time_series) > max_timestamps:\n",
    "        max_timestamps = len(current_time_series)\n",
    "        best_offset = offset\n",
    "\n",
    "print(f\"Best Offset: {best_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in time_series_list:\n",
    "    for i in range(0, 59):\n",
    "        \n",
    "        for df_chunk in time_series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and sort dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_temp_air(df) :\n",
    "    upper = 80\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = 0\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 600\n",
    "    lower = 100 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "\n",
    "def sep_cap_diff(df) :\n",
    "    column_name=df.columns[4:]\n",
    "    sensor_columns = column_name\n",
    "    sensor_columnsPC1 = sensor_columns[::2]\n",
    "    sensor_columnsPC2 = sensor_columns[1::2]\n",
    "    reldiff = 0.5\n",
    "    diff = pd.DataFrame()\n",
    "    allzero = pd.DataFrame()\n",
    "    fitting = df.copy()\n",
    "    for i in range(len(sensor_columnsPC1)):    \n",
    "        close = (np.isclose(fitting[sensor_columnsPC1[i]], \\\n",
    "                           fitting[sensor_columnsPC2[i]], rtol=reldiff) & (fitting[sensor_columnsPC1[i]] != 0) & (fitting[sensor_columnsPC2[i]] != 0))\n",
    "        invalid = ((fitting[sensor_columnsPC1[i]] == 0) & (fitting[sensor_columnsPC2[i]] == 0))\n",
    "        diff = diff._append(fitting[~close])\n",
    "        allzero = allzero._append(fitting[invalid])\n",
    "        fitting = fitting[close & ~invalid]\n",
    "        print(\"dshape : \",  diff.shape)\n",
    "    diff['different'] = True\n",
    "    allzero['allzero'] = True\n",
    "    return fitting, diff, allzero\n",
    "    \n",
    "df = pd.read_csv(file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = h\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "\n",
    "fit, diff, az = sep_cap_diff(fit)\n",
    "print(\"Sensor diff : \",fit.shape, diff.shape, az.shape)\n",
    "ruled_out= ruled_out._append(diff)\n",
    "ruled_out= ruled_out._append(az)\n",
    "ruled_out = ruled_out[~ruled_out.index.duplicated(keep='first')]\n",
    "print(\"ruled out df : \",ruled_out.shape)\n",
    "\n",
    "fit.to_csv(dirpath/'fitting.csv',sep=';')\n",
    "ruled_out.to_csv(dirpath/'ruled_out.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamps_UTC' column to datetime type\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# Sort the DataFrame by the 'timestamps_UTC' column\n",
    "df_sorted = df.sort_values(by='timestamps_UTC')\n",
    "\n",
    "# new_file = 'sorted_ar41_for_ulb.csv'\n",
    "\n",
    "# # Save the sorted DataFrame to a new CSV file\n",
    "# df_sorted.to_csv(repertory+new_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sorted = df\n",
    "# Select the first 100 rows\n",
    "df_subset = df_sorted.head(N)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_column = df_subset.isnull().sum()\n",
    "\n",
    "# Check if there are any missing values in the entire DataFrame\n",
    "any_missing_values = df_subset.isnull().values.any()\n",
    "\n",
    "total_missing_values = missing_values_column.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing values in each column:\\n\", missing_values_column)\n",
    "print(\"\\nAre there any missing values in the DataFrame?\", any_missing_values)\n",
    "print(\"Nombre total de valeurs manquantes dans le DataFrame:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_subset = df_subset.iloc[:1000]\n",
    "\n",
    "current_set = df_sub_subset\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = current_set.interpolate()\n",
    "\n",
    "# Check if there are any missing values after interpolation\n",
    "any_missing_values_after_interpolation = df_interpolated.isnull().values.any()\n",
    "\n",
    "# Display the results\n",
    "print(\"Are there any missing values in the DataFrame after interpolation?\", any_missing_values_after_interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values using polynomial interpolation of order 2\n",
    "df_interpolated_polynomial = current_set.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Original Data\n",
    "plt.scatter(range(len(current_set)), current_set['RS_E_InAirTemp_PC2'], label='Original', marker='o', color='blue', alpha=0.5)\n",
    "\n",
    "# Linear Interpolation\n",
    "# plt.plot(range(len(df_interpolated)), df_interpolated['RS_E_InAirTemp_PC2'], label='Linear Interpolation', color='green')\n",
    "\n",
    "# Polynomial Interpolation\n",
    "plt.plot(range(len(df_interpolated_polynomial)), df_interpolated_polynomial['RS_E_InAirTemp_PC2'], label='Polynomial Interpolation', color='orange')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('RS_E_InAirTemp_PC2')\n",
    "plt.title('Comparison of Interpolations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_linear = current_set['RS_E_InAirTemp_PC2'] - df_interpolated['RS_E_InAirTemp_PC2']\n",
    "residuals_polynomial = current_set['RS_E_InAirTemp_PC2'] - df_interpolated_polynomial['RS_E_InAirTemp_PC2']\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(len(current_set)), residuals_linear, label='Linear Residuals', color='green')\n",
    "plt.plot(range(len(current_set)), residuals_polynomial, label='Polynomial Residuals', color='orange')\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1, label='Zero Residual')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['mapped_veh_id'].nunique()\n",
    "print(f'Total number of unique values: {unique_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "def plot_autocorrelation(df, column_name, lags=40):\n",
    "    \"\"\"\n",
    "    Plot autocorrelation for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - column_name (str): The name of the column for which to plot the autocorrelation.\n",
    "    - lags (int): Number of lags to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(df[column_name], lags=lags)\n",
    "    plt.title(f'Autocorrelation Plot for {column_name}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "selected_columns = current_set.columns[5:]\n",
    "print(selected_columns)\n",
    "\n",
    "# selected_columns = current_set.columns[6]\n",
    "for column_name in selected_columns:\n",
    "    plot_autocorrelation(current_set, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stationarity or non stationarity of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from 5th to 8th (index 4 to 7)\n",
    "selected_columns = current_set.columns[5:]\n",
    "\n",
    "for column_name in selected_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(current_set.index, current_set[column_name], label=column_name)\n",
    "    plt.title(f'Data Plot for {column_name}')\n",
    "    plt.xlabel('Index')  # Use 'Index' instead of 'Timestamp'\n",
    "    plt.ylabel(column_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
