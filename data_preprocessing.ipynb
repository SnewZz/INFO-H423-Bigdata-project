{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertory = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ar41_for_ulb.csv'\n",
    "N = 2**13\n",
    "B = 2**6\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weather data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weather data from openweathermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# import json\n",
    "# import csv\n",
    "# import time\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # Timestamp 1 january 2023 UTC  is\t1672531201\n",
    "# # Each hour, + 3600\n",
    "# # Each day, + 86400\n",
    "# # End, 15 september, UTC is 1694818801\n",
    "# # Duration : 258 days or 22287600 seconds\n",
    "# # Careful, changement d'heure happening in Belgium but not for UTC\n",
    "\n",
    "# file = 'csvWeather.csv'\n",
    "# with open(repertory+file,'w',newline='') as weather :\n",
    "#     writer = csv.writer(weather)\n",
    "#     # Write the columns names on the csv\n",
    "#     fields = ['date','time','temp','wind','humidity']\n",
    "#     writer.writerow(fields)\n",
    "#     # For Bxl, Gand and Bastogne, write temperature, wind speed and humidity for each hour and each day between 01/01/23 and 15/09/23\n",
    "#     for day in range(258):\n",
    "#         for hour in range(24):\n",
    "#             urlbx = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.5&lon=4.20&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbx))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             dt = datetime.datetime.fromtimestamp(data['list'][0]['dt'])\n",
    "#             dt = str(dt).split()\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlgand = 'https://history.openweathermap.org/data/2.5/history/city?lat=51.03&lon=3.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlgand))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlbast = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.0&lon=5.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbast))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine our dataset with weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest point to define which weather report is the most relevant \n",
    "def closest(point, locs):\n",
    "    close = 9e20\n",
    "    for j in locs :\n",
    "        dist = 0\n",
    "        for i in range (2) :\n",
    "            dist += (point[i]-j[i])**2\n",
    "        if np.sqrt(dist) < close :\n",
    "            close = np.sqrt(dist)\n",
    "            best = j\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWeather = repertory+'csvWeather.csv'\n",
    "pathTrain = repertory+'ar41_for_ulb.csv'\n",
    "weather = pd.read_csv(pathWeather)\n",
    "train = pd.read_csv(pathTrain,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series for each dataframe that contains the closest point of the 3 studied locations\n",
    "# We chose 3 different towns in Belgium based on their average temperatures. \n",
    "# One in the south, rather cold, one in the center and one in the north, rather warm.\n",
    "locs = [(50.5, 4.2),(51.0,3.43),(50.0,5.43)]\n",
    "weather['closest'] = list(zip(weather['lat'], weather['lon']))\n",
    "train['coord'] = list(zip(train['lat'], train['lon']))\n",
    "train['closest'] = train.apply(lambda row: closest(row['coord'],locs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = train['timestamps_UTC'].str.split().str[0]\n",
    "train['time'] = train['timestamps_UTC'].str.split().str[1].str[:2]\n",
    "weather['time'] = pd.to_datetime(weather['time'], format='%H:%M:%S')\n",
    "weather['time'] = weather['time'].dt.hour.apply(lambda x: str(x).zfill(2))\n",
    "print(train.shape,weather.shape)\n",
    "final = pd.merge(train,weather,on=['time','date','closest'],how='inner')\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('final2.xlsx',engine='xlsxwriter') #TODO\n",
    "final2 = final[final['mapped_veh_id'] < 105]\n",
    "final2.to_excel(writer,sheet_name='sheet1')\n",
    "writer.close()\n",
    "final.to_csv(repertory+'trains_weather.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv with weather data\n",
    "file = 'trains_weather.csv'\n",
    "N = 17679273\n",
    "df = pd.read_csv(repertory+file, sep=';')\n",
    "df.info()\n",
    "df = df.iloc[:, :-2].drop(df.columns[16:18], axis=1) #remove some useless columns (closest,lat_y,lon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''use file to make fit & ruled_out through rule-based processing.\n",
    "fitting is not sorted by trains or by time'''\n",
    "\n",
    "def sep_temp_air(df) :\n",
    "    upper = 200\n",
    "    lower = -15 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = -5 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 200\n",
    "    lower = -100 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = -1\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 689\n",
    "    lower = 1 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "file = 'trains_weather.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+ file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "N = df.shape[0]\n",
    "df = df.dropna() #12000 NaN's in the PC2 columns\n",
    "print(\"Naaan : \", df.shape[0],N-df.shape[0])\n",
    "ruled_out = df.isna()\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.to_csv(repertory+'fittingtst.csv',sep=';',index=False)\n",
    "ruled_out.to_csv(repertory+'ruled_outtst.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fittingtst.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize time difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timezone\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "#remove nan\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "time_diffs = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "\n",
    "custom_bins = np.concatenate([np.arange(0, 130, 1)])\n",
    "\n",
    "# Plot a histogram of the time differences with custom bin edges\n",
    "plt.hist(time_diffs, bins=custom_bins, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Time Differences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time series by train and with regular interval of time between two consecutive timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des variables\n",
    "time_series_list = [] \n",
    "current_time_series = [] \n",
    "\n",
    "for i in tqdm(range(len(df)), desc=\"Processing Rows\", unit=\" row\"):\n",
    "    # Add first row\n",
    "    if not current_time_series:\n",
    "        current_time_series.append(df.iloc[i])\n",
    "    else:\n",
    "        # check change of train or difference of time between 2 rows\n",
    "        train_id_change = df['mapped_veh_id'].iloc[i] != df['mapped_veh_id'].iloc[i - 1]\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds()\n",
    "\n",
    "        if train_id_change or time_diff > 150: \n",
    "            # Add current time serie to result\n",
    "            time_series_list.append(current_time_series)\n",
    "            # start new time serie\n",
    "            current_time_series = [df.iloc[i]]\n",
    "        else:\n",
    "            # add row to current time serie\n",
    "            current_time_series.append(df.iloc[i])\n",
    "\n",
    "# add last time serie to result\n",
    "if current_time_series:\n",
    "    time_series_list.append(current_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(time_series_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que j'ai ce code j'aimerais pour chaque timeseries, parcourir chacune de leur rows et prendre leur timestamps, les arrondir à la seconde la plus proche, ensuite j'aimerais appliquer un modulo 60 sur ce timestamps et le reste de ce modulo (qui va donc de 0 à 59) représente l'index du nouveau tableau que je veux créer où on va mettre la current row. Cela me permettra ainsi de voir à quel offset il y a le plus de rows avec des intervals de 60 secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Supposons que votre DataFrame s'appelle df\n",
    "# Supposons également que vous avez déjà créé time_series_list comme indiqué dans le code précédent\n",
    "\n",
    "# Créer un tableau global pour stocker les résultats\n",
    "global_table = []\n",
    "\n",
    "# Utiliser tqdm pour ajouter une barre de progression\n",
    "for time_series in tqdm(time_series_list, desc=\"Processing Time Series\", unit=\" time series\"):\n",
    "    # Créer un tableau \"current\" pour stocker les occurrences\n",
    "    current_table = np.zeros(60, dtype=int)\n",
    "\n",
    "    for row in time_series:\n",
    "        # Prendre le timestamp, l'arrondir à la seconde la plus proche et appliquer le modulo 60\n",
    "        rounded_timestamp = round(row['timestamps_UTC'].timestamp())\n",
    "        index = int(rounded_timestamp % 60)\n",
    "\n",
    "        # Incrémenter la valeur dans le tableau \"current\"\n",
    "        current_table[index] += 1\n",
    "\n",
    "    # Trouver l'index où la valeur est la plus grande dans le tableau \"current\"\n",
    "    max_index = np.argmax(current_table)\n",
    "\n",
    "    # Ajouter l'index au tableau global\n",
    "    global_table.append((max_index, current_table[max_index]))\n",
    "\n",
    "# Maintenant, global_table contient les résultats pour chaque time series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store globale_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "df.insert(3,'n_n1',n_n1)\n",
    "\n",
    "df2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vince\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774711                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                14  \n",
      "1      7  274.73  2.76        91  51.0  3.43                22  \n",
      "2      7  274.73  2.76        91  51.0  3.43                43  \n",
      "3      7  274.73  2.76        91  51.0  3.43                47  \n",
      "4      7  274.73  2.76        91  51.0  3.43                16  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                20  \n",
      "96    14  276.90  4.75        82  51.0  3.43                14  \n",
      "97    14  276.90  4.75        82  51.0  3.43                20  \n",
      "98    14  276.90  4.75        82  51.0  3.43                15  \n",
      "99    14  276.90  4.75        82  51.0  3.43                21  \n",
      "\n",
      "[100 rows x 27 columns]\n",
      "    traj            timestamps_UTC  n_n1\n",
      "0      0 2023-01-23 07:25:08+00:00   0.0\n",
      "1      0 2023-01-23 07:25:16+00:00   8.0\n",
      "2      0 2023-01-23 07:25:37+00:00  21.0\n",
      "3      0 2023-01-23 07:25:41+00:00   4.0\n",
      "4      0 2023-01-23 07:26:10+00:00  29.0\n",
      "..   ...                       ...   ...\n",
      "95     1 2023-01-23 14:52:14+00:00   6.0\n",
      "96     1 2023-01-23 14:53:08+00:00  54.0\n",
      "97     1 2023-01-23 14:53:14+00:00   6.0\n",
      "98     1 2023-01-23 14:54:09+00:00  55.0\n",
      "99     1 2023-01-23 14:54:15+00:00   6.0\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774711                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                14  \n",
      "1      7  274.73  2.76        91  51.0  3.43                22  \n",
      "2      7  274.73  2.76        91  51.0  3.43                43  \n",
      "3      7  274.73  2.76        91  51.0  3.43                47  \n",
      "4      7  274.73  2.76        91  51.0  3.43                16  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                20  \n",
      "96    14  276.90  4.75        82  51.0  3.43                14  \n",
      "97    14  276.90  4.75        82  51.0  3.43                20  \n",
      "98    14  276.90  4.75        82  51.0  3.43                15  \n",
      "99    14  276.90  4.75        82  51.0  3.43                21  \n",
      "\n",
      "[100 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import timezone\n",
    "\n",
    "df2['bin_interval_time'] = ((df2['timestamps_UTC'] - pd.datetime.now(tz=pytz.UTC)).dt.total_seconds() % 60).astype(int) #TODO check\n",
    "print(df2.head(100))\n",
    "\n",
    "\n",
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain2 = df2.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain2['sequence_start'] = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain2['traj'] = sortedByTrain2['sequence_start'].cumsum()\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1']].head(100))\n",
    "\n",
    "sortedByTrain2['most_common_bin'] = sortedByTrain2.groupby('traj')['bin_interval_time'].transform(lambda x: x.mode().iloc[0])\n",
    "\n",
    "\n",
    "\n",
    "print(df2.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     traj            timestamps_UTC  n_n1  bin_interval_time  most_common_bin\n",
      "0       0 2023-01-23 07:25:08+00:00   0.0                 14               14\n",
      "1       0 2023-01-23 07:25:16+00:00   8.0                 22               14\n",
      "4       0 2023-01-23 07:26:10+00:00  29.0                 16               14\n",
      "5       0 2023-01-23 07:26:11+00:00   1.0                 17               14\n",
      "6       0 2023-01-23 07:27:09+00:00  58.0                 15               14\n",
      "..    ...                       ...   ...                ...              ...\n",
      "100     1 2023-01-23 14:55:08+00:00  53.0                 14               18\n",
      "101     1 2023-01-23 14:55:15+00:00   7.0                 21               18\n",
      "102     1 2023-01-23 14:56:07+00:00  52.0                 13               18\n",
      "103     1 2023-01-23 14:56:11+00:00   4.0                 17               18\n",
      "104     1 2023-01-23 14:57:08+00:00  57.0                 14               18\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = sortedByTrain2.copy()\n",
    "tau = 12\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['bin_interval_time'] >= sortedByTrain2['most_common_bin']-tau) & (sortedByTrain2['bin_interval_time'] <= sortedByTrain2['most_common_bin']+tau))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16517984\n",
      "13985929\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2 = df3\n",
    "tau = 12\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['n_n1'] >= 60-tau) & (sortedByTrain2['n_n1'] <= 60+tau)) | (sortedByTrain2['n_n1'] >= 120-tau) & (sortedByTrain2['n_n1'] <= 120+tau)]\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop time intervals \n",
    "tau = 12\n",
    "print(df.head(100))\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.head(100))\n",
    "df.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain['timestamps_UTC'] = pd.to_datetime(sortedByTrain['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain['sequence_start'] = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain['traj'] = sortedByTrain['sequence_start'].cumsum()\n",
    "print(sortedByTrain['traj'].drop_duplicates())\n",
    "sortedByTrain = sortedByTrain.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(502))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=sortedByTrain.columns)\n",
    "print(df3)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df3),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "def addrow(row,df3) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df3.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df3.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df3.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.shape)\n",
    "df = pd.concat([sortedByTrain,df3])\n",
    "print(sortedByTrain.shape)\n",
    "print(df.shape)\n",
    "df=df.interpolate()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain.to_csv(dirpath/'trajets_train.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain = sortedByTrain[['mapped_veh_id','timestamps_UTC','traj']]\n",
    "meantrajlength = sortedByTrain.groupby('traj').count().agg('mean')\n",
    "print(meantrajlength)\n",
    "print(sortedByTrain[sortedByTrain['traj'] == 183182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of time series (we consider a new time series when, between two timestamps, the train change or the difference of time is greater than 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Use tqdm to add a progress bar\n",
    "for i in tqdm(range(1, len(df)), desc=\"Processing Rows\", unit=\" row\"):\n",
    "    # Check if it's the same train\n",
    "    if df['mapped_veh_id'].iloc[i] == df['mapped_veh_id'].iloc[i - 1]:\n",
    "        # Calculate time difference in seconds\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds()\n",
    "\n",
    "        # Check for gaps greater than 5 minutes\n",
    "        if time_diff > 300:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc(i))\n",
    "    # Create new time series if it is a new train\n",
    "    else : \n",
    "        if current_time_series:\n",
    "            time_series_list.append(current_time_series)\n",
    "        current_time_series = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Iterate over possible starting points (offsets)\n",
    "max_timestamps = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in tqdm(range(60), desc=\"Searching for Best Offset\", unit=\"second\"):\n",
    "    current_time_series = []\n",
    "    current_time_series.append(df.head(1))\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for i in range(1, len(df)):\n",
    "        # Calculate time difference in seconds with the offset\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds() + offset\n",
    "\n",
    "        # Check for gaps greater than 60 seconds\n",
    "        if time_diff > 60:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc[i])\n",
    "\n",
    "    # Create new time series if it is a new train\n",
    "    if current_time_series:\n",
    "        time_series_list.append(current_time_series)\n",
    "\n",
    "    # Update best offset if the current offset gives more timestamps\n",
    "    if len(current_time_series) > max_timestamps:\n",
    "        max_timestamps = len(current_time_series)\n",
    "        best_offset = offset\n",
    "\n",
    "print(f\"Best Offset: {best_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in time_series_list:\n",
    "    for i in range(0, 59):\n",
    "        \n",
    "        for df_chunk in time_series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and sort dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_temp_air(df) :\n",
    "    upper = 80\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = 0\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 600\n",
    "    lower = 100 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "\n",
    "def sep_cap_diff(df) :\n",
    "    column_name=df.columns[4:]\n",
    "    sensor_columns = column_name\n",
    "    sensor_columnsPC1 = sensor_columns[::2]\n",
    "    sensor_columnsPC2 = sensor_columns[1::2]\n",
    "    reldiff = 0.5\n",
    "    diff = pd.DataFrame()\n",
    "    allzero = pd.DataFrame()\n",
    "    fitting = df.copy()\n",
    "    for i in range(len(sensor_columnsPC1)):    \n",
    "        close = (np.isclose(fitting[sensor_columnsPC1[i]], \\\n",
    "                           fitting[sensor_columnsPC2[i]], rtol=reldiff) & (fitting[sensor_columnsPC1[i]] != 0) & (fitting[sensor_columnsPC2[i]] != 0))\n",
    "        invalid = ((fitting[sensor_columnsPC1[i]] == 0) & (fitting[sensor_columnsPC2[i]] == 0))\n",
    "        diff = diff._append(fitting[~close])\n",
    "        allzero = allzero._append(fitting[invalid])\n",
    "        fitting = fitting[close & ~invalid]\n",
    "        print(\"dshape : \",  diff.shape)\n",
    "    diff['different'] = True\n",
    "    allzero['allzero'] = True\n",
    "    return fitting, diff, allzero\n",
    "    \n",
    "df = pd.read_csv(file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = h\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "\n",
    "fit, diff, az = sep_cap_diff(fit)\n",
    "print(\"Sensor diff : \",fit.shape, diff.shape, az.shape)\n",
    "ruled_out= ruled_out._append(diff)\n",
    "ruled_out= ruled_out._append(az)\n",
    "ruled_out = ruled_out[~ruled_out.index.duplicated(keep='first')]\n",
    "print(\"ruled out df : \",ruled_out.shape)\n",
    "\n",
    "fit.to_csv(dirpath/'fitting.csv',sep=';')\n",
    "ruled_out.to_csv(dirpath/'ruled_out.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamps_UTC' column to datetime type\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# Sort the DataFrame by the 'timestamps_UTC' column\n",
    "df_sorted = df.sort_values(by='timestamps_UTC')\n",
    "\n",
    "# new_file = 'sorted_ar41_for_ulb.csv'\n",
    "\n",
    "# # Save the sorted DataFrame to a new CSV file\n",
    "# df_sorted.to_csv(repertory+new_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sorted = df\n",
    "# Select the first 100 rows\n",
    "df_subset = df_sorted.head(N)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_column = df_subset.isnull().sum()\n",
    "\n",
    "# Check if there are any missing values in the entire DataFrame\n",
    "any_missing_values = df_subset.isnull().values.any()\n",
    "\n",
    "total_missing_values = missing_values_column.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing values in each column:\\n\", missing_values_column)\n",
    "print(\"\\nAre there any missing values in the DataFrame?\", any_missing_values)\n",
    "print(\"Nombre total de valeurs manquantes dans le DataFrame:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_subset = df_subset.iloc[:1000]\n",
    "\n",
    "current_set = df_sub_subset\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = current_set.interpolate()\n",
    "\n",
    "# Check if there are any missing values after interpolation\n",
    "any_missing_values_after_interpolation = df_interpolated.isnull().values.any()\n",
    "\n",
    "# Display the results\n",
    "print(\"Are there any missing values in the DataFrame after interpolation?\", any_missing_values_after_interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values using polynomial interpolation of order 2\n",
    "df_interpolated_polynomial = current_set.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Original Data\n",
    "plt.scatter(range(len(current_set)), current_set['RS_E_InAirTemp_PC2'], label='Original', marker='o', color='blue', alpha=0.5)\n",
    "\n",
    "# Linear Interpolation\n",
    "# plt.plot(range(len(df_interpolated)), df_interpolated['RS_E_InAirTemp_PC2'], label='Linear Interpolation', color='green')\n",
    "\n",
    "# Polynomial Interpolation\n",
    "plt.plot(range(len(df_interpolated_polynomial)), df_interpolated_polynomial['RS_E_InAirTemp_PC2'], label='Polynomial Interpolation', color='orange')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('RS_E_InAirTemp_PC2')\n",
    "plt.title('Comparison of Interpolations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_linear = current_set['RS_E_InAirTemp_PC2'] - df_interpolated['RS_E_InAirTemp_PC2']\n",
    "residuals_polynomial = current_set['RS_E_InAirTemp_PC2'] - df_interpolated_polynomial['RS_E_InAirTemp_PC2']\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(len(current_set)), residuals_linear, label='Linear Residuals', color='green')\n",
    "plt.plot(range(len(current_set)), residuals_polynomial, label='Polynomial Residuals', color='orange')\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1, label='Zero Residual')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['mapped_veh_id'].nunique()\n",
    "print(f'Total number of unique values: {unique_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "def plot_autocorrelation(df, column_name, lags=40):\n",
    "    \"\"\"\n",
    "    Plot autocorrelation for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - column_name (str): The name of the column for which to plot the autocorrelation.\n",
    "    - lags (int): Number of lags to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(df[column_name], lags=lags)\n",
    "    plt.title(f'Autocorrelation Plot for {column_name}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "selected_columns = current_set.columns[5:]\n",
    "print(selected_columns)\n",
    "\n",
    "# selected_columns = current_set.columns[6]\n",
    "for column_name in selected_columns:\n",
    "    plot_autocorrelation(current_set, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stationarity or non stationarity of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from 5th to 8th (index 4 to 7)\n",
    "selected_columns = current_set.columns[5:]\n",
    "\n",
    "for column_name in selected_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(current_set.index, current_set[column_name], label=column_name)\n",
    "    plt.title(f'Data Plot for {column_name}')\n",
    "    plt.xlabel('Index')  # Use 'Index' instead of 'Timestamp'\n",
    "    plt.ylabel(column_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
