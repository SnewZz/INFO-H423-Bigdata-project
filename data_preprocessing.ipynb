{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertory = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ar41_for_ulb.csv'\n",
    "N = 2**13\n",
    "B = 2**6\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add weather data to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get weather data from openweathermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.request import urlopen\n",
    "# import json\n",
    "# import csv\n",
    "# import time\n",
    "# import datetime\n",
    "\n",
    "\n",
    "# # Timestamp 1 january 2023 UTC  is\t1672531201\n",
    "# # Each hour, + 3600\n",
    "# # Each day, + 86400\n",
    "# # End, 15 september, UTC is 1694818801\n",
    "# # Duration : 258 days or 22287600 seconds\n",
    "# # Careful, changement d'heure happening in Belgium but not for UTC\n",
    "\n",
    "# file = 'csvWeather.csv'\n",
    "# with open(repertory+file,'w',newline='') as weather :\n",
    "#     writer = csv.writer(weather)\n",
    "#     # Write the columns names on the csv\n",
    "#     fields = ['date','time','temp','wind','humidity']\n",
    "#     writer.writerow(fields)\n",
    "#     # For Bxl, Gand and Bastogne, write temperature, wind speed and humidity for each hour and each day between 01/01/23 and 15/09/23\n",
    "#     for day in range(258):\n",
    "#         for hour in range(24):\n",
    "#             urlbx = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.5&lon=4.20&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbx))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             dt = datetime.datetime.fromtimestamp(data['list'][0]['dt'])\n",
    "#             dt = str(dt).split()\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlgand = 'https://history.openweathermap.org/data/2.5/history/city?lat=51.03&lon=3.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlgand))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])\n",
    "\n",
    "#             urlbast = 'https://history.openweathermap.org/data/2.5/history/city?lat=50.0&lon=5.43&start={}&cnt=1&appid=0ecabdca455f4f022ca58f1a5929a9b7'.format(1672531201+(day*86400)+(hour*3600))\n",
    "#             data = json.load(urlopen(urlbast))\n",
    "#             windspeed = data['list'][0]['wind']['speed']\n",
    "#             temp = data['list'][0]['main']['temp']\n",
    "#             humidity =  data['list'][0]['main']['humidity']\n",
    "#             writer.writerow([dt[0],dt[1],temp,windspeed,humidity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine our dataset with weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest point to define which weather report is the most relevant \n",
    "def closest(point, locs):\n",
    "    close = 9e20\n",
    "    for j in locs :\n",
    "        dist = 0\n",
    "        for i in range (2) :\n",
    "            dist += (point[i]-j[i])**2\n",
    "        if np.sqrt(dist) < close :\n",
    "            close = np.sqrt(dist)\n",
    "            best = j\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWeather = repertory+'csvWeather.csv'\n",
    "pathTrain = repertory+'ar41_for_ulb.csv'\n",
    "weather = pd.read_csv(pathWeather)\n",
    "train = pd.read_csv(pathTrain,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a series for each dataframe that contains the closest point of the 3 studied locations\n",
    "# We chose 3 different towns in Belgium based on their average temperatures. \n",
    "# One in the south, rather cold, one in the center and one in the north, rather warm.\n",
    "locs = [(50.5, 4.2),(51.0,3.43),(50.0,5.43)]\n",
    "weather['closest'] = list(zip(weather['lat'], weather['lon']))\n",
    "train['coord'] = list(zip(train['lat'], train['lon']))\n",
    "train['closest'] = train.apply(lambda row: closest(row['coord'],locs),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['date'] = train['timestamps_UTC'].str.split().str[0]\n",
    "train['time'] = train['timestamps_UTC'].str.split().str[1].str[:2]\n",
    "weather['time'] = pd.to_datetime(weather['time'], format='%H:%M:%S')\n",
    "weather['time'] = weather['time'].dt.hour.apply(lambda x: str(x).zfill(2))\n",
    "print(train.shape,weather.shape)\n",
    "final = pd.merge(train,weather,on=['time','date','closest'],how='inner')\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('final2.xlsx',engine='xlsxwriter') #TODO\n",
    "final2 = final[final['mapped_veh_id'] < 105]\n",
    "final2.to_excel(writer,sheet_name='sheet1')\n",
    "writer.close()\n",
    "final.to_csv(repertory+'trains_weather.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read csv with weather data\n",
    "file = 'trains_weather.csv'\n",
    "N = 17679273\n",
    "df = pd.read_csv(repertory+file, sep=';')\n",
    "df.info()\n",
    "df = df.iloc[:, :-2].drop(df.columns[16:18], axis=1) #remove some useless columns (closest,lat_y,lon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''use file to make fit & ruled_out through rule-based processing.\n",
    "fitting is not sorted by trains or by time'''\n",
    "\n",
    "def sep_temp_air(df) :\n",
    "    upper = 200\n",
    "    lower = -15 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = -5 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 200\n",
    "    lower = -100 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = -1\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 689\n",
    "    lower = 1 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "file = 'trains_weather.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+ file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "N = df.shape[0]\n",
    "df = df.dropna() #12000 NaN's in the PC2 columns\n",
    "print(\"Naaan : \", df.shape[0],N-df.shape[0])\n",
    "ruled_out = df.isna()\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out.append(h)\n",
    "ruled_out = ruled_out.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.to_csv(repertory+'fittingtst.csv',sep=';',index=False)\n",
    "ruled_out.to_csv(repertory+'ruled_outtst.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'fittingtst.csv'\n",
    "\n",
    "df = pd.read_csv(repertory+file, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize time difference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert timezone\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "#remove nan\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print(df.shape)\n",
    "\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "time_diffs = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "\n",
    "custom_bins = np.concatenate([np.arange(0, 130, 1)])\n",
    "\n",
    "# Plot a histogram of the time differences with custom bin edges\n",
    "plt.hist(time_diffs, bins=custom_bins, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel('Time Differences (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Time Differences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time series by train and with regular interval of time between two consecutive timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "df = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = df['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "df.insert(3,'n_n1',n_n1)\n",
    "\n",
    "df2 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vince\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774711                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                 8  \n",
      "1      7  274.73  2.76        91  51.0  3.43                16  \n",
      "2      7  274.73  2.76        91  51.0  3.43                37  \n",
      "3      7  274.73  2.76        91  51.0  3.43                41  \n",
      "4      7  274.73  2.76        91  51.0  3.43                10  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                14  \n",
      "96    14  276.90  4.75        82  51.0  3.43                 8  \n",
      "97    14  276.90  4.75        82  51.0  3.43                14  \n",
      "98    14  276.90  4.75        82  51.0  3.43                 9  \n",
      "99    14  276.90  4.75        82  51.0  3.43                15  \n",
      "\n",
      "[100 rows x 27 columns]\n",
      "    traj            timestamps_UTC  n_n1\n",
      "0      0 2023-01-23 07:25:08+00:00   0.0\n",
      "1      0 2023-01-23 07:25:16+00:00   8.0\n",
      "2      0 2023-01-23 07:25:37+00:00  21.0\n",
      "3      0 2023-01-23 07:25:41+00:00   4.0\n",
      "4      0 2023-01-23 07:26:10+00:00  29.0\n",
      "..   ...                       ...   ...\n",
      "95     1 2023-01-23 14:52:14+00:00   6.0\n",
      "96     1 2023-01-23 14:53:08+00:00  54.0\n",
      "97     1 2023-01-23 14:53:14+00:00   6.0\n",
      "98     1 2023-01-23 14:54:09+00:00  55.0\n",
      "99     1 2023-01-23 14:54:15+00:00   6.0\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "       index  Unnamed: 0.1  mapped_veh_id  n_n1            timestamps_UTC  \\\n",
      "0   10519630       5493376          102.0   0.0 2023-01-23 07:25:08+00:00   \n",
      "1   10519642       8969009          102.0   8.0 2023-01-23 07:25:16+00:00   \n",
      "2   10519662      13873566          102.0  21.0 2023-01-23 07:25:37+00:00   \n",
      "3   10519663      14994675          102.0   4.0 2023-01-23 07:25:41+00:00   \n",
      "4   10519656      11935795          102.0  29.0 2023-01-23 07:26:10+00:00   \n",
      "..       ...           ...            ...   ...                       ...   \n",
      "95  10539750       3691099          102.0   6.0 2023-01-23 14:52:14+00:00   \n",
      "96  10540524       8960631          102.0  54.0 2023-01-23 14:53:08+00:00   \n",
      "97  10539849       4395399          102.0   6.0 2023-01-23 14:53:14+00:00   \n",
      "98  10539500       2406388          102.0  55.0 2023-01-23 14:54:09+00:00   \n",
      "99  10541729      16102555          102.0   6.0 2023-01-23 14:54:15+00:00   \n",
      "\n",
      "        lat_x     lon_x  RS_E_InAirTemp_PC1  RS_E_InAirTemp_PC2  \\\n",
      "0   51.017864  3.769079                17.0                18.0   \n",
      "1   51.017875  3.769046                17.0                20.0   \n",
      "2   51.017208  3.770179                19.0                20.0   \n",
      "3   51.016916  3.771036                19.0                20.0   \n",
      "4   51.016503  3.772182                19.0                21.0   \n",
      "..        ...       ...                 ...                 ...   \n",
      "95  51.015739  3.774153                 6.0                12.0   \n",
      "96  51.015514  3.774711                 6.0                12.0   \n",
      "97  51.015749  3.774149                 6.0                12.0   \n",
      "98  51.015499  3.774702                 6.0                12.0   \n",
      "99  51.015740  3.774146                 6.0                12.0   \n",
      "\n",
      "    RS_E_OilPress_PC1  ...                    coord       closest        date  \\\n",
      "0               210.0  ...  (51.0178637, 3.7690786)  (51.0, 3.43)  2023-01-23   \n",
      "1               200.0  ...  (51.0178749, 3.7690464)  (51.0, 3.43)  2023-01-23   \n",
      "2               193.0  ...  (51.0172078, 3.7701787)  (51.0, 3.43)  2023-01-23   \n",
      "3               196.0  ...  (51.0169155, 3.7710357)  (51.0, 3.43)  2023-01-23   \n",
      "4               200.0  ...  (51.0165029, 3.7721823)  (51.0, 3.43)  2023-01-23   \n",
      "..                ...  ...                      ...           ...         ...   \n",
      "95              276.0  ...   (51.0157393, 3.774153)  (51.0, 3.43)  2023-01-23   \n",
      "96              269.0  ...  (51.0155138, 3.7747105)  (51.0, 3.43)  2023-01-23   \n",
      "97              276.0  ...  (51.0157493, 3.7741489)  (51.0, 3.43)  2023-01-23   \n",
      "98              269.0  ...  (51.0154994, 3.7747023)  (51.0, 3.43)  2023-01-23   \n",
      "99              269.0  ...  (51.0157403, 3.7741459)  (51.0, 3.43)  2023-01-23   \n",
      "\n",
      "    time    temp  wind  humidity lat_y lon_y bin_interval_time  \n",
      "0      7  274.73  2.76        91  51.0  3.43                 8  \n",
      "1      7  274.73  2.76        91  51.0  3.43                16  \n",
      "2      7  274.73  2.76        91  51.0  3.43                37  \n",
      "3      7  274.73  2.76        91  51.0  3.43                41  \n",
      "4      7  274.73  2.76        91  51.0  3.43                10  \n",
      "..   ...     ...   ...       ...   ...   ...               ...  \n",
      "95    14  276.90  4.75        82  51.0  3.43                14  \n",
      "96    14  276.90  4.75        82  51.0  3.43                 8  \n",
      "97    14  276.90  4.75        82  51.0  3.43                14  \n",
      "98    14  276.90  4.75        82  51.0  3.43                 9  \n",
      "99    14  276.90  4.75        82  51.0  3.43                15  \n",
      "\n",
      "[100 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "from datetime import timezone\n",
    "\n",
    "now = pd.datetime.now(tz=pytz.UTC)\n",
    "\n",
    "\n",
    "# Convert the timestamps_UTC column to datetime objects\n",
    "df2['timestamps_UTC'] = pd.to_datetime(df2['timestamps_UTC'], errors='coerce')\n",
    "\n",
    "# Create a new column 'bin_interval_time' containing the seconds part\n",
    "df2['bin_interval_time'] = df2['timestamps_UTC'].dt.second\n",
    "\n",
    "print(df2.head(100))\n",
    "\n",
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain2 = df2.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain2['sequence_start'] = sortedByTrain2['n_n1'] > 150 \n",
    "\n",
    "sortedByTrain2['traj'] = sortedByTrain2['sequence_start'].cumsum()\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1']].head(100))\n",
    "\n",
    "sortedByTrain2['most_common_bin'] = sortedByTrain2.groupby('traj')['bin_interval_time'].transform(lambda x: x.mode().iloc[0])\n",
    "\n",
    "print(df2.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vince\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "sortedByTrain2 = sortedByTrain\n",
    "sortedByTrain2[\"diff_bin\"] = abs(sortedByTrain2[\"bin_interval_time\"] - sortedByTrain2[\"most_common_bin\"])\n",
    "\n",
    "tau = 10\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2[sortedByTrain2[\"diff_bin\"] <= tau]\n",
    "\n",
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)\n",
    "\n",
    "sortedByTrain2 = sortedByTrain2[sortedByTrain2[\"n_n1\"] >= 2 * tau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16517984\n",
      "9466653\n"
     ]
    }
   ],
   "source": [
    "print(sortedByTrain.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timestamps_UTC   n_n1  bin_interval_time  most_common_bin\n",
      "7201940 2023-05-04 09:40:04+00:00  236.0                  4                8\n",
      "7201941 2023-05-04 09:41:04+00:00   60.0                  4                8\n",
      "7201942 2023-05-04 09:42:11+00:00   67.0                 11                8\n",
      "7201943 2023-05-04 09:43:05+00:00   54.0                  5                8\n",
      "7201944 2023-05-04 09:44:06+00:00   61.0                  6                8\n",
      "7201946 2023-05-04 09:45:07+00:00   61.0                  7                8\n",
      "7201948 2023-05-04 09:46:07+00:00   60.0                  7                8\n",
      "7201951 2023-05-04 09:50:11+00:00  244.0                 11                8\n",
      "7201952 2023-05-04 09:51:11+00:00   60.0                 11                8\n",
      "7201953 2023-05-04 09:52:11+00:00   60.0                 11                8\n",
      "7201954 2023-05-04 09:53:01+00:00   50.0                  1                8\n",
      "7201955 2023-05-04 09:54:04+00:00   63.0                  4                8\n",
      "7201959 2023-05-04 09:57:15+00:00  191.0                 15                8\n",
      "7201961 2023-05-04 09:58:07+00:00   52.0                  7                8\n",
      "7201963 2023-05-04 09:59:08+00:00   61.0                  8                8\n",
      "7201964 2023-05-04 10:00:02+00:00   54.0                  2                8\n",
      "7201966 2023-05-04 10:01:03+00:00   55.0                  3                8\n",
      "7201968 2023-05-04 10:02:03+00:00   55.0                  3                8\n",
      "7201970 2023-05-04 10:03:02+00:00   54.0                  2                8\n",
      "7201972 2023-05-04 10:04:03+00:00   52.0                  3                8\n",
      "7201974 2023-05-04 10:05:06+00:00   55.0                  6                8\n",
      "7201976 2023-05-04 10:06:07+00:00   55.0                  7                8\n",
      "7201978 2023-05-04 10:07:07+00:00   56.0                  7                8\n",
      "7201980 2023-05-04 10:08:07+00:00   56.0                  7                8\n",
      "7201982 2023-05-04 10:09:07+00:00   53.0                  7                8\n",
      "7201984 2023-05-04 10:10:00+00:00   46.0                  0                8\n",
      "7201986 2023-05-04 10:11:00+00:00   45.0                  0                8\n",
      "7201988 2023-05-04 10:12:00+00:00   45.0                  0                8\n",
      "7201990 2023-05-04 10:13:01+00:00   46.0                  1                8\n",
      "7201992 2023-05-04 10:14:00+00:00   52.0                  0                8\n"
     ]
    }
   ],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20100]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level_0                                16517982\n",
       "index                                  16517983\n",
       "Unnamed: 0.1                           17679269\n",
       "n_n1                                1.08894e+07\n",
       "mapped_veh_id                               197\n",
       "timestamps_UTC        2023-09-13 21:52:58+00:00\n",
       "lat_x                                   51.3029\n",
       "lon_x                                   6.57248\n",
       "RS_E_InAirTemp_PC1                           91\n",
       "RS_E_InAirTemp_PC2                           95\n",
       "RS_E_OilPress_PC1                           688\n",
       "RS_E_OilPress_PC2                           688\n",
       "RS_E_RPM_PC1                               2303\n",
       "RS_E_RPM_PC2                               2463\n",
       "RS_E_WatTemp_PC1                            100\n",
       "RS_E_WatTemp_PC2                            100\n",
       "RS_T_OilTemp_PC1                            127\n",
       "RS_T_OilTemp_PC2                            116\n",
       "coord                   (51.3028883, 4.3449666)\n",
       "closest                            (51.0, 3.43)\n",
       "temp                                     305.62\n",
       "wind                                      13.89\n",
       "humidity                                    100\n",
       "lat_y                                        51\n",
       "lon_y                                      5.43\n",
       "bin_interval_time                            59\n",
       "traj                                      47812\n",
       "most_common_bin                              59\n",
       "diff_bin                                     10\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedByTrain2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['level_0', 'index', 'Unnamed: 0.1', 'mapped_veh_id', 'timestamps_UTC',\n",
      "       'lat_x', 'lon_x', 'RS_E_InAirTemp_PC1', 'RS_E_InAirTemp_PC2',\n",
      "       'RS_E_OilPress_PC1', 'RS_E_OilPress_PC2', 'RS_E_RPM_PC1',\n",
      "       'RS_E_RPM_PC2', 'RS_E_WatTemp_PC1', 'RS_E_WatTemp_PC2',\n",
      "       'RS_T_OilTemp_PC1', 'RS_T_OilTemp_PC2', 'coord', 'closest', 'temp',\n",
      "       'wind', 'humidity', 'lat_y', 'lon_y', 'bin_interval_time', 'traj',\n",
      "       'most_common_bin'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tau\n",
    "tau = 10\n",
    "\n",
    "# Create a new dataframe for the regular time intervals\n",
    "df_regular_intervals = pd.DataFrame(columns=sortedByTrain2.columns)\n",
    "\n",
    "# Iterate through each row in the original dataframe\n",
    "for i in range(len(sortedByTrain2)):\n",
    "    current_row = sortedByTrain2.iloc[i]\n",
    "\n",
    "    # Check the condition for the timestamps_UTC difference\n",
    "    if i == 0 or (current_row['timestamps_UTC'] - prev_row['timestamps_UTC']).seconds in range(60 - tau, 60 + tau + 1):\n",
    "        \n",
    "        # Check if the timestamps_UTC is close to most_common_bin\n",
    "        bin_diff = abs((current_row['timestamps_UTC'] - pd.Timestamp(current_row['most_common_bin'])).seconds)\n",
    "        if bin_diff <= tau:\n",
    "            df_regular_intervals = df_regular_intervals.append(current_row, ignore_index=True)\n",
    "\n",
    "    # Update the previous row\n",
    "    prev_row = current_row\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(df_regular_intervals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = sortedByTrain2.copy()\n",
    "tau = 10\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['bin_interval_time'] >= sortedByTrain2['most_common_bin']-tau) & (sortedByTrain2['bin_interval_time'] <= sortedByTrain2['most_common_bin']+tau))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.drop(\"n_n1\", inplace=True, axis = 1)\n",
    "print(sortedByTrain2.columns)\n",
    "# Sort the DataFrame by train_id and timestamps in chronological order\n",
    "sortedByTrain2 = sortedByTrain2.sort_values(by=['mapped_veh_id','timestamps_UTC'])\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain2['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain2.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2.to_csv('./result/clean1.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'clean1.csv'\n",
    "\n",
    "sortedByTrain2 = pd.read_csv(repertory+file, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the timestamps_UTC column to datetime objects\n",
    "sortedByTrain2['timestamps_UTC'] = pd.to_datetime(sortedByTrain2['timestamps_UTC'], errors='coerce')\n",
    "\n",
    "# Create a new column 'seconds' containing the seconds part\n",
    "sortedByTrain2['seconds'] = sortedByTrain2['timestamps_UTC'].dt.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\",\"seconds\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop time intervals \n",
    "tau = 10\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "print(sortedByTrain2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns=sortedByTrain2.columns)\n",
    "print(df_final)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df_final),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "    \n",
    "def addrow(row,df_final) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df_final.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df_final.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df_final.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain2.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = sortedByTrain2[sortedByTrain2[\"traj\"]==20200]\n",
    "print(tmp[[\"timestamps_UTC\", \"n_n1\", \"bin_interval_time\",\"most_common_bin\"]].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[sortedByTrain2[\"traj\"]==20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain2 = df3\n",
    "tau = 12\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2 = sortedByTrain2[((sortedByTrain2['n_n1'] >= 60-tau) & (sortedByTrain2['n_n1'] <= 60+tau)) | (sortedByTrain2['n_n1'] >= 120-tau) & (sortedByTrain2['n_n1'] <= 120+tau)]\n",
    "print(sortedByTrain2.head(100))\n",
    "sortedByTrain2.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain2[['traj','timestamps_UTC','n_n1','bin_interval_time','most_common_bin']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#drop time intervals \n",
    "tau = 12\n",
    "print(df.head(100))\n",
    "df = df[((df['n_n1'] >= 60-tau) & (df['n_n1'] <= 60+tau)) | (df['n_n1'] >= 120-tau) & (df['n_n1'] <= 120+tau)]\n",
    "print(df.head(100))\n",
    "df.drop(\"n_n1\", axis = 1,  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by train_id and timestamps again\n",
    "sortedByTrain = df.sort_values(by=['mapped_veh_id','timestamps_UTC']).reset_index()\n",
    "\n",
    "# Convert timestamps to datetime objects\n",
    "sortedByTrain['timestamps_UTC'] = pd.to_datetime(sortedByTrain['timestamps_UTC'], utc=True)\n",
    "\n",
    "# Calculate time differences (T(n)) between consecutive samples and fill NaN with 0\n",
    "n_n1 = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds()\n",
    "n_n1 = n_n1.to_frame(name='n_n1')\n",
    "n_n1 = n_n1.fillna(0)\n",
    "\n",
    "# Insert the calculated time differences as a new column 'n_n1' at position 3\n",
    "sortedByTrain.insert(3,'n_n1',n_n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'sequence_start' column based on time gaps greater than 150 seconds\n",
    "sortedByTrain['sequence_start'] = sortedByTrain['timestamps_UTC'].diff().dt.total_seconds() > 150  #Can we not use the n__n1 column?\n",
    "\n",
    "sortedByTrain['traj'] = sortedByTrain['sequence_start'].cumsum()\n",
    "print(sortedByTrain['traj'].drop_duplicates())\n",
    "sortedByTrain = sortedByTrain.drop(['sequence_start','date','time'],axis = 1)\n",
    "\n",
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sortedByTrain[['traj','timestamps_UTC','n_n1']].head(502))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(columns=sortedByTrain.columns)\n",
    "print(df3)\n",
    "global add \n",
    "add = -1 \n",
    "\n",
    "def gb_to_rows(gb) :\n",
    "    gb.apply(lambda w: addrow(w,df3),axis=1)\n",
    "    print(gb['traj'].drop_duplicates())\n",
    "def addrow(row,df3) :\n",
    "    global add\n",
    "    if row['n_n1'] > 90 :\n",
    "        df3.loc[add,'timestamps_UTC'] = row['timestamps_UTC'] + dt.timedelta(seconds=60)\n",
    "        df3.loc[add,'mapped_veh_id'] = row['mapped_veh_id']\n",
    "        df3.loc[add,'traj'] = row['traj']\n",
    "        add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sortedByTrain.groupby('traj').apply(lambda z: gb_to_rows(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.shape)\n",
    "df = pd.concat([sortedByTrain,df3])\n",
    "print(sortedByTrain.shape)\n",
    "print(df.shape)\n",
    "df=df.interpolate()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain.to_csv(dirpath/'trajets_train.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByTrain = sortedByTrain[['mapped_veh_id','timestamps_UTC','traj']]\n",
    "meantrajlength = sortedByTrain.groupby('traj').count().agg('mean')\n",
    "print(meantrajlength)\n",
    "print(sortedByTrain[sortedByTrain['traj'] == 183182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of time series (we consider a new time series when, between two timestamps, the train change or the difference of time is greater than 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Use tqdm to add a progress bar\n",
    "for i in tqdm(range(1, len(df)), desc=\"Processing Rows\", unit=\" row\"):\n",
    "    # Check if it's the same train\n",
    "    if df['mapped_veh_id'].iloc[i] == df['mapped_veh_id'].iloc[i - 1]:\n",
    "        # Calculate time difference in seconds\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds()\n",
    "\n",
    "        # Check for gaps greater than 5 minutes\n",
    "        if time_diff > 300:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc(i))\n",
    "    # Create new time series if it is a new train\n",
    "    else : \n",
    "        if current_time_series:\n",
    "            time_series_list.append(current_time_series)\n",
    "        current_time_series = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# sorting + drop na\n",
    "# Conversion du fuseau horaire\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'], utc=True)\n",
    "tzone = timezone('Europe/Paris')\n",
    "df['timestamps_UTC'] = df['timestamps_UTC'].dt.tz_convert(tzone)\n",
    "\n",
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Tri du DataFrame par 'mapped_veh_id' puis 'timestamps_UTC'\n",
    "df = df.sort_values(by=['mapped_veh_id', 'timestamps_UTC'])\n",
    "\n",
    "# Initialize variables\n",
    "time_series_list = []\n",
    "current_time_series = []\n",
    "current_time_series.append(df.head(1))\n",
    "\n",
    "# Iterate over possible starting points (offsets)\n",
    "max_timestamps = 0\n",
    "best_offset = 0\n",
    "\n",
    "for offset in tqdm(range(60), desc=\"Searching for Best Offset\", unit=\"second\"):\n",
    "    current_time_series = []\n",
    "    current_time_series.append(df.head(1))\n",
    "\n",
    "    # Use tqdm to add a progress bar\n",
    "    for i in range(1, len(df)):\n",
    "        # Calculate time difference in seconds with the offset\n",
    "        time_diff = (df['timestamps_UTC'].iloc[i] - df['timestamps_UTC'].iloc[i - 1]).total_seconds() + offset\n",
    "\n",
    "        # Check for gaps greater than 60 seconds\n",
    "        if time_diff > 60:\n",
    "            # If gap is too large, start a new time series\n",
    "            if current_time_series:\n",
    "                time_series_list.append(current_time_series)\n",
    "            current_time_series = []\n",
    "        else:\n",
    "            current_time_series.append(df.iloc[i])\n",
    "\n",
    "    # Create new time series if it is a new train\n",
    "    if current_time_series:\n",
    "        time_series_list.append(current_time_series)\n",
    "\n",
    "    # Update best offset if the current offset gives more timestamps\n",
    "    if len(current_time_series) > max_timestamps:\n",
    "        max_timestamps = len(current_time_series)\n",
    "        best_offset = offset\n",
    "\n",
    "print(f\"Best Offset: {best_offset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_series in time_series_list:\n",
    "    for i in range(0, 59):\n",
    "        \n",
    "        for df_chunk in time_series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and sort dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_temp_air(df) :\n",
    "    upper = 80\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_InAirTemp_PC1\"] > upper) | (df['RS_E_InAirTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high] #Getting only rows that show an upper error\n",
    "    higher['hta'] = True #Adding a column to indicate if there's an upper error\n",
    "\n",
    "    low = (df[\"RS_E_InAirTemp_PC1\"] < lower) | (df['RS_E_InAirTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lta'] = True\n",
    "\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_water(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_E_WatTemp_PC1\"] > upper) | (df['RS_E_WatTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['htw'] = True\n",
    "    low = (df[\"RS_E_WatTemp_PC1\"] <= lower) | (df['RS_E_WatTemp_PC2'] <= lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['ltw'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_temp_oil(df) :\n",
    "    upper = 100\n",
    "    lower = 0 # Determine if it is an overflow (and should be removed) or not\n",
    "    high = (df[\"RS_T_OilTemp_PC1\"] > upper) | (df['RS_T_OilTemp_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hto'] = True\n",
    "    low = (df[\"RS_T_OilTemp_PC1\"] < lower) | (df['RS_T_OilTemp_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lto'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_rpm(df) :\n",
    "    upper = 3000\n",
    "    lower = 0\n",
    "    high = (df[\"RS_E_RPM_PC1\"] > upper) | (df['RS_E_RPM_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hrpm'] = True\n",
    "    low = (df[\"RS_E_RPM_PC1\"] < lower) | (df['RS_E_RPM_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lrpm'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "def sep_press(df) :\n",
    "    upper = 600\n",
    "    lower = 100 \n",
    "    high = (df[\"RS_E_OilPress_PC1\"] > upper) | (df['RS_E_OilPress_PC2'] > upper)\n",
    "    higher = df.copy()[high]\n",
    "    higher['hpress'] = True\n",
    "    low = (df[\"RS_E_OilPress_PC1\"] < lower) | (df['RS_E_OilPress_PC2'] < lower)\n",
    "    lower = df.copy()[low]\n",
    "    lower['lpress'] = True\n",
    "    fitting = df[~(high | low)]\n",
    "    return fitting, higher, lower\n",
    "\n",
    "\n",
    "def sep_cap_diff(df) :\n",
    "    column_name=df.columns[4:]\n",
    "    sensor_columns = column_name\n",
    "    sensor_columnsPC1 = sensor_columns[::2]\n",
    "    sensor_columnsPC2 = sensor_columns[1::2]\n",
    "    reldiff = 0.5\n",
    "    diff = pd.DataFrame()\n",
    "    allzero = pd.DataFrame()\n",
    "    fitting = df.copy()\n",
    "    for i in range(len(sensor_columnsPC1)):    \n",
    "        close = (np.isclose(fitting[sensor_columnsPC1[i]], \\\n",
    "                           fitting[sensor_columnsPC2[i]], rtol=reldiff) & (fitting[sensor_columnsPC1[i]] != 0) & (fitting[sensor_columnsPC2[i]] != 0))\n",
    "        invalid = ((fitting[sensor_columnsPC1[i]] == 0) & (fitting[sensor_columnsPC2[i]] == 0))\n",
    "        diff = diff._append(fitting[~close])\n",
    "        allzero = allzero._append(fitting[invalid])\n",
    "        fitting = fitting[close & ~invalid]\n",
    "        print(\"dshape : \",  diff.shape)\n",
    "    diff['different'] = True\n",
    "    allzero['allzero'] = True\n",
    "    return fitting, diff, allzero\n",
    "    \n",
    "df = pd.read_csv(file,sep=\";\",index_col=[0])   \n",
    "print(df.head())\n",
    "\n",
    "\n",
    "fit,h,l = sep_temp_air(df)\n",
    "print(\"Temperature air : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = h\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_oil(fit)\n",
    "print(\"Temperature oil : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_temp_water(fit)\n",
    "print(\"Temperature water : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_press(fit)\n",
    "print(\"Pressure : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "fit,h,l = sep_rpm(fit)\n",
    "print(\"RPM : \", fit.shape, h.shape, l.shape)\n",
    "ruled_out = ruled_out._append(h)\n",
    "ruled_out = ruled_out._append(l)\n",
    "\n",
    "\n",
    "fit, diff, az = sep_cap_diff(fit)\n",
    "print(\"Sensor diff : \",fit.shape, diff.shape, az.shape)\n",
    "ruled_out= ruled_out._append(diff)\n",
    "ruled_out= ruled_out._append(az)\n",
    "ruled_out = ruled_out[~ruled_out.index.duplicated(keep='first')]\n",
    "print(\"ruled out df : \",ruled_out.shape)\n",
    "\n",
    "fit.to_csv(dirpath/'fitting.csv',sep=';')\n",
    "ruled_out.to_csv(dirpath/'ruled_out.csv',sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'timestamps_UTC' column to datetime type\n",
    "df['timestamps_UTC'] = pd.to_datetime(df['timestamps_UTC'])\n",
    "\n",
    "# Sort the DataFrame by the 'timestamps_UTC' column\n",
    "df_sorted = df.sort_values(by='timestamps_UTC')\n",
    "\n",
    "# new_file = 'sorted_ar41_for_ulb.csv'\n",
    "\n",
    "# # Save the sorted DataFrame to a new CSV file\n",
    "# df_sorted.to_csv(repertory+new_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sorted = df\n",
    "# Select the first 100 rows\n",
    "df_subset = df_sorted.head(N)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_column = df_subset.isnull().sum()\n",
    "\n",
    "# Check if there are any missing values in the entire DataFrame\n",
    "any_missing_values = df_subset.isnull().values.any()\n",
    "\n",
    "total_missing_values = missing_values_column.sum()\n",
    "\n",
    "# Display the results\n",
    "print(\"Missing values in each column:\\n\", missing_values_column)\n",
    "print(\"\\nAre there any missing values in the DataFrame?\", any_missing_values)\n",
    "print(\"Nombre total de valeurs manquantes dans le DataFrame:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_subset = df_subset.iloc[:1000]\n",
    "\n",
    "current_set = df_sub_subset\n",
    "# Interpolate missing values using linear interpolation\n",
    "df_interpolated = current_set.interpolate()\n",
    "\n",
    "# Check if there are any missing values after interpolation\n",
    "any_missing_values_after_interpolation = df_interpolated.isnull().values.any()\n",
    "\n",
    "# Display the results\n",
    "print(\"Are there any missing values in the DataFrame after interpolation?\", any_missing_values_after_interpolation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate missing values using polynomial interpolation of order 2\n",
    "df_interpolated_polynomial = current_set.interpolate(method='polynomial', order=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Original Data\n",
    "plt.scatter(range(len(current_set)), current_set['RS_E_InAirTemp_PC2'], label='Original', marker='o', color='blue', alpha=0.5)\n",
    "\n",
    "# Linear Interpolation\n",
    "# plt.plot(range(len(df_interpolated)), df_interpolated['RS_E_InAirTemp_PC2'], label='Linear Interpolation', color='green')\n",
    "\n",
    "# Polynomial Interpolation\n",
    "plt.plot(range(len(df_interpolated_polynomial)), df_interpolated_polynomial['RS_E_InAirTemp_PC2'], label='Polynomial Interpolation', color='orange')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Row number')\n",
    "plt.ylabel('RS_E_InAirTemp_PC2')\n",
    "plt.title('Comparison of Interpolations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_linear = current_set['RS_E_InAirTemp_PC2'] - df_interpolated['RS_E_InAirTemp_PC2']\n",
    "residuals_polynomial = current_set['RS_E_InAirTemp_PC2'] - df_interpolated_polynomial['RS_E_InAirTemp_PC2']\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(range(len(current_set)), residuals_linear, label='Linear Residuals', color='green')\n",
    "plt.plot(range(len(current_set)), residuals_polynomial, label='Polynomial Residuals', color='orange')\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=1, label='Zero Residual')\n",
    "\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['mapped_veh_id'].nunique()\n",
    "print(f'Total number of unique values: {unique_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "def plot_autocorrelation(df, column_name, lags=40):\n",
    "    \"\"\"\n",
    "    Plot autocorrelation for a specific column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - column_name (str): The name of the column for which to plot the autocorrelation.\n",
    "    - lags (int): Number of lags to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(df[column_name], lags=lags)\n",
    "    plt.title(f'Autocorrelation Plot for {column_name}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()\n",
    "\n",
    "selected_columns = current_set.columns[5:]\n",
    "print(selected_columns)\n",
    "\n",
    "# selected_columns = current_set.columns[6]\n",
    "for column_name in selected_columns:\n",
    "    plot_autocorrelation(current_set, column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check stationarity or non stationarity of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns from 5th to 8th (index 4 to 7)\n",
    "selected_columns = current_set.columns[5:]\n",
    "\n",
    "for column_name in selected_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(current_set.index, current_set[column_name], label=column_name)\n",
    "    plt.title(f'Data Plot for {column_name}')\n",
    "    plt.xlabel('Index')  # Use 'Index' instead of 'Timestamp'\n",
    "    plt.ylabel(column_name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
